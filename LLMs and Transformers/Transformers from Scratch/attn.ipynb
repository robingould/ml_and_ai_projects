{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute self attention for some input sequence.\n",
    "For every token, we produce a new sequence of encoding vectors vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(input_sequence):\n",
    "    output = np.zeros(shape=input_sequence.shape)\n",
    "\n",
    "    # iterate over each token in input sequence\n",
    "    # compute attention score between this token, and all others\n",
    "    for i, pivot_vector in enumerate(input_sequence):\n",
    "        scores = np.zeros(shape=(len(input_sequence),))\n",
    "        for j, vector in enumerate(input_sequence):\n",
    "            # compute attention score with dot product between tokens\n",
    "            scores[j] = np.dot(pivot_vector, vector.T)\n",
    "        scores /= np.sqrt(input_sequence.shape[1])\n",
    "        scores = nn.Softmax(scores)\n",
    "        new_pivot_representation = np.zeros(shape=pivot_vector.shape)\n",
    "\n",
    "        # sum all tokens weighted by attention score, which we return\n",
    "        for j, vector in enumerate(input_sequence):\n",
    "            new_pivot_representation += vector * scores[j]\n",
    "        output[i] = new_pivot_representation\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take input sequence, of some length and some embedding vector size. Transform this into 3 tensors (query, key, value) of dimension sequence length by embedding vector size. Multiply by weight tensors of embedding vector size by embedding vector size. This gives us tensors with dimensions of the sequence length by the size of the embedding vector which we split along the embedding dimension into the amount of heads we want. Each of these serves as a head of the multi-headed attention and is the sized by the ratio between embedding vector size and the amount of desired heads. Each head has access to the full input sequence, but is limited in the embeddings it can see for each token. Softmax the different groups of heads together by which embedding vector they have access to, then finally concatenate the heads and multiply them by the weight tensor of the dimension of the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, all dimensions actually include batch dimension\n",
    "# for each tensor, the dimensions look more like (sequence_batch, d_model, etc...)\n",
    "# as we do a batch for each sequence\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model # embedding vector size\n",
    "        self.n_heads = n_heads # number of heads\n",
    "        assert d_model % n_heads == 0, \"embedding vector size (d_model) needs to be divisible by the nubmer of heads (n_heads)\"\n",
    "        self.d_k = d_model // n_heads # size of each head\n",
    "        self.d_v = self.d_k # in attention is all you need, this is called d_v, but its equal to d_k\n",
    "\n",
    "        # the 3 weight tensors of embedding vector size by embedding vector size\n",
    "        self.w_q = nn.Linear(d_model, d_model) # query weight tensor\n",
    "        self.w_k = nn.Linear(d_model, d_model) # key weight tensor\n",
    "        self.w_v = nn.Linear(d_model, d_model) # value weight tensor\n",
    "\n",
    "        # weight tensor to multiply by the concatenated heads at the end\n",
    "        self.w_concat = nn.Linear(n_heads * self.d_v, d_model)\n",
    "\n",
    "        # dropout\n",
    "        self.Dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # Scaled Dot-Product Attention\n",
    "    def attention(Q, K, V, mask, dropout):\n",
    "        # K/keys has dimension (batch num, num heads, sequence length, d_k)\n",
    "        # input V has dimension d_v (fairly certain its the same)\n",
    "        # last element in keys dimensions\n",
    "        d_k = K.shape[-1] \n",
    "        # query multiplied by transpose of last 2 dimensions of key\n",
    "        # divided by the square root of d_k (temperature) \n",
    "        # helps to prevent dot products from growing in magnitude causing vanishing gradients\n",
    "        attn = nn.matmul(Q / np.sqrt(d_k), K.transpose(2, 3))\n",
    "\n",
    "        # masking\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn = nn.Softmax(attn, dim = -1)\n",
    "\n",
    "        # dropout\n",
    "        if dropout is not None:\n",
    "            attn = dropout(attn)\n",
    "\n",
    "        output = nn.matmul(attn, V)\n",
    "        # return output and attention scores\n",
    "        return output, attn\n",
    "        \n",
    "\n",
    "    # Masking, replacing values we don't want to interact. Default is no mask.\n",
    "    # If mask is applied softmax is applied puts them to 0. \n",
    "    # Hides attention of those tokens. Otherwise just gets values\n",
    "    # for each token with each other token. \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # multiply each tensor with its weight tensor\n",
    "        query_tensor = self.w_q(Q)\n",
    "        key_tensor = self.w_k(K)\n",
    "        value_tensor = self.w_v(V)\n",
    "\n",
    "        # split by the number of heads\n",
    "        query_tensor = self.split(query_tensor)\n",
    "        key_tensor = self.split(key_tensor)\n",
    "        value_tensor = self.split(value_tensor)\n",
    "\n",
    "        # get attention \n",
    "        output, self.attention_scores = self.attention(query_tensor, key_tensor, value_tensor, mask, self.Dropout)\n",
    "        \n",
    "        # concatenating, contiguous for in place in memory\n",
    "        output = output.transpose(1, 2).contiguous().view(output.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # multiply by output tensor\n",
    "        return self.w_concat(output)\n",
    "    \n",
    "    # Helper function to do logic of splitting by number of heads\n",
    "    def split(self, tensor):\n",
    "        \n",
    "        # dimensions go from (batch num, sequence length, d_model) to (batch num, number of heads, sequence legnth, size of each head)\n",
    "        split_output = tensor.view(tensor.shape[0], tensor.shape[1], self.n_heads, self.d_k).transpose(1, 2)\n",
    "        return split_output\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformershw2-7zABzyjs-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00848d74a619cba88b19ed4cd70f61d379f903de608676376e8cb007e7d9600a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
