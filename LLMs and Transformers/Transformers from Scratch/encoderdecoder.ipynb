{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, all dimensions actually include batch dimension\n",
    "# for each tensor, the dimensions look more like (sequence_batch, d_model, etc...)\n",
    "# as we do a batch for each sequence\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model # embedding vector size\n",
    "        self.n_heads = n_heads # number of heads\n",
    "        assert d_model % n_heads == 0, \"embedding vector size (d_model) needs to be divisible by the nubmer of heads (n_heads)\"\n",
    "        self.d_k = d_model // n_heads # size of each head\n",
    "        self.d_v = self.d_k # in attention is all you need, this is called d_v, but its equal to d_k\n",
    "\n",
    "        # the 3 weight tensors of embedding vector size by embedding vector size\n",
    "        self.w_q = nn.Linear(d_model, d_model) # query weight tensor\n",
    "        self.w_k = nn.Linear(d_model, d_model) # key weight tensor\n",
    "        self.w_v = nn.Linear(d_model, d_model) # value weight tensor\n",
    "\n",
    "        # weight tensor to multiply by the concatenated heads at the end\n",
    "        self.w_concat = nn.Linear(n_heads * self.d_v, d_model)\n",
    "\n",
    "        # dropout\n",
    "        self.Dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # Scaled Dot-Product Attention\n",
    "    def attention(Q, K, V, mask, dropout):\n",
    "        # K/keys has dimension (batch num, num heads, sequence length, d_k)\n",
    "        # input V has dimension d_v (fairly certain its the same)\n",
    "        # last element in keys dimensions\n",
    "        d_k = K.shape[-1] \n",
    "        # query multiplied by transpose of last 2 dimensions of key\n",
    "        # divided by the square root of d_k (temperature) \n",
    "        # helps to prevent dot products from growing in magnitude causing vanishing gradients\n",
    "        attn = nn.matmul(Q / np.sqrt(d_k), K.transpose(2, 3))\n",
    "\n",
    "        # masking\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn = nn.Softmax(attn, dim = -1)\n",
    "\n",
    "        # dropout\n",
    "        if dropout is not None:\n",
    "            attn = dropout(attn)\n",
    "\n",
    "        output = nn.matmul(attn, V)\n",
    "        # return output and attention scores\n",
    "        return output, attn\n",
    "        \n",
    "\n",
    "    # Masking, replacing values we don't want to interact. Default is no mask.\n",
    "    # If mask is applied softmax is applied puts them to 0. \n",
    "    # Hides attention of those tokens. Otherwise just gets values\n",
    "    # for each token with each other token. \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # multiply each tensor with its weight tensor\n",
    "        query_tensor = self.w_q(Q)\n",
    "        key_tensor = self.w_k(K)\n",
    "        value_tensor = self.w_v(V)\n",
    "\n",
    "        # split by the number of heads\n",
    "        query_tensor = self.split(query_tensor)\n",
    "        key_tensor = self.split(key_tensor)\n",
    "        value_tensor = self.split(value_tensor)\n",
    "\n",
    "        # get attention \n",
    "        output, self.attention_scores = self.attention(query_tensor, key_tensor, value_tensor, mask, self.Dropout)\n",
    "        \n",
    "        # concatenating, contiguous for in place in memory\n",
    "        output = output.transpose(1, 2).contiguous().view(output.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # multiply by output tensor\n",
    "        return self.w_concat(output)\n",
    "    \n",
    "    # Helper function to do logic of splitting by number of heads\n",
    "    def split(self, tensor):\n",
    "        \n",
    "        # dimensions go from (batch num, sequence length, d_model) to (batch num, number of heads, sequence legnth, size of each head)\n",
    "        split_output = tensor.view(tensor.shape[0], tensor.shape[1], self.n_heads, self.d_k).transpose(1, 2)\n",
    "        return split_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each layer in the encoder and decoder has a position-wise feed forward network. This is basically just 2 linear translations with ReLU between. The two linear transformations use different parameters per layer. Goes from d_model  -> d_ff -> d_model at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionFeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float)->None:\n",
    "        super(PositionFeedForward, self).__init__()\n",
    "        \n",
    "        #W1 and b1\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #W2 and b2\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) \n",
    "\n",
    "    # FFN(x) = max(0, xW1 + b1)W2 + b2\n",
    "    def forward(self, x):\n",
    "        return self.linear_2(self.dropout(nn.ReLU(self.linear_1(x))))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1607.06450.pdf\n",
    "Layer Normalization\n",
    "\n",
    "Find the mean and std deviation, basically just valuing the sequence and features of one batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eeps: float = -1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.eeps = eeps\n",
    "        self.a = nn.Parameter(nn.ones(d_model))\n",
    "        self.b = nn.Parameter(nn.zeros(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        std = x.std(dim = -1, keepdim = True)\n",
    "        return self.a * (x - mean) / (std + self.eeps) + self.b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack 6 layers, each with 2 sublayers. Everything comes from positional encoding, and goes into the multi-head self attention sublayer, which outputs into feed forward. Everything has dropout, and residual connections from each output and goes into add with layernorm of the outputs of the multi-head and feed forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ff, n_heads, dropout) -> None:\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, n_heads=n_heads)\n",
    "        self.norm1 = LayerNorm(d_model=d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.feed_forward = PositionFeedForward(d_model=d_model, d_ff=ff, dropout=dropout)\n",
    "        self.norm2 = LayerNorm(d_model=d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        # multi attention\n",
    "\n",
    "        # get residual\n",
    "        residual = x\n",
    "        x = self.attention(Q=x, K=x, V=x, mask=src_mask)\n",
    "\n",
    "        # add & norm\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + residual)\n",
    "\n",
    "        # get other residual\n",
    "        residual = x\n",
    "        x = self.feed_forward(x)\n",
    "\n",
    "        # add & norm\n",
    "        x = self.dropout2(x)\n",
    "        return self.norm2(x + residual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through all Encoder layers, layernorm the end result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers, d_model, ff, n_heads, dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model,\n",
    "                                                  ff=ff,\n",
    "                                                  n_heads=n_heads,\n",
    "                                                  dropout=dropout)\n",
    "                                     for _ in range(layers)])\n",
    "        self.norm = LayerNorm()\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Similarly, has 6 stacks but with 3 sublayers. The main difference from encoder is the extra multi-head attention group. \n",
    "\n",
    "Query comes from output embedding/second positional encoding\n",
    "Key and value comes from output of Encoder.\n",
    "\n",
    "Each has residual connections added back in in the add & norm stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ff, n_heads, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.mask_attention = MultiHeadAttention(d_model=d_model, n_heads=n_heads)\n",
    "        self.norm1 = LayerNorm(d_model=d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, n_heads=n_heads)\n",
    "        self.norm2 = LayerNorm(d_model=d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.feed_forward = PositionFeedForward(d_model=d_model, d_ff=ff, dropout=dropout)\n",
    "        self.norm3 = LayerNorm(d_model=d_model)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "    \n",
    "    \n",
    "    def forward(self, pos_out, encoder_out, encoder_mask, target_mask):\n",
    "        # position encoding outputs as residual \n",
    "        residual = pos_out\n",
    "        # put into attention block\n",
    "        x = self.mask_attention(Q=pos_out, K=pos_out, V=pos_out, mask=target_mask)\n",
    "\n",
    "        # add & norm\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + residual)\n",
    "        \n",
    "        # encoder decoder attention, in second multihead attention\n",
    "        residual = x\n",
    "        x = self.attention(Q=pos_out, K=encoder_out, V=encoder_out, mask=encoder_mask)\n",
    "\n",
    "        # add & norm\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x + residual)\n",
    "\n",
    "        # feed forwards\n",
    "        residual = x\n",
    "        x = self.feed_forward(x)\n",
    "\n",
    "        # add & norm\n",
    "        x = self.dropout3(x)\n",
    "        return self.norm3(x + residual)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, loop through the decoder layers on each of their masks, and layernorm the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers, d_model, ff, n_heads, dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model,\n",
    "                                                  ff=ff,\n",
    "                                                  n_heads=n_heads,\n",
    "                                                  dropout=dropout)\n",
    "                                     for _ in range(layers)])\n",
    "        self.norm = LayerNorm()\n",
    "\n",
    "    \n",
    "    # target is decoder, source is encoder\n",
    "    def forward(self, target, source, target_mask, source_mask):\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            target = layer(target, source, target_mask, source_mask)\n",
    "        output = self.norm(target)\n",
    "        return output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformershw2-7zABzyjs-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00848d74a619cba88b19ed4cd70f61d379f903de608676376e8cb007e7d9600a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
