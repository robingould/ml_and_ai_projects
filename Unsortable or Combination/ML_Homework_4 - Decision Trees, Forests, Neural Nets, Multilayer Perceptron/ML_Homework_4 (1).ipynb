{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 1. Decision Trees and Ensemble Learning\n"
      ],
      "metadata": {
        "id": "bFaiZ2mvSU7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. (i) Given the binary classification problem from Homework 1, where we use a linear decision boundary not perpendicular to either the x1 or x2 axis, we would not be able to use a shallow decision tree with a small depth, because you can't sufficiently approximate the slope using a small number of perpendicular splits of the data. With only a small numbrer of splits, we have a zig zagging boundary."
      ],
      "metadata": {
        "id": "N81wBNxiSgbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. (ii) if we rotated the data, such that the linear split between the data was either horizontal or vertical, then we could use a single split decision tree to recover the decision boundary."
      ],
      "metadata": {
        "id": "ZhCQiaZbTNe2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\n",
        "- Random forests select to split along some random subset (typically $\\sqrt n$ rounded down) of the features to generate each tree using random sampling with replacement.\n",
        "- Gradient Boosting modifies the labels using residuals of previous learners to train stronger learner.\n",
        "- AdaBoost uses a higher probability to pick samples that are missclassified by previous trees. It modifies the weights of the datapoints in order to higher weight the data that performs poorly."
      ],
      "metadata": {
        "id": "O-CiGjqLT9iv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. (i) Switching to\n",
        "\n",
        "```\n",
        "X[\"random_cat\"] = rng.randint(4, size=X.shape[0])\n",
        "```\n",
        "Changing 3 to 4 increases the feature importance from below 0.05 to above 0.05. Our train and test accuracies stay roughly the same.\n"
      ],
      "metadata": {
        "id": "HiFIgPGSXcBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. (ii) When features are correlated, you get the same information from both features. This means that when we permute one feature, it has little effect on the model's performance due to the existence of that information in a coorelated feature, this make it seem like the permuation imporance is low."
      ],
      "metadata": {
        "id": "GlqMc9Rua1vm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2. Training Neural Networks"
      ],
      "metadata": {
        "id": "1tA6hYRLckGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The sigmoid function captures the outcomes of binary classification as a probability, we are projecting the neural network outputs onto a sigmoid curve, between 0 and 1, which is easily interpreble as the probability of being in the positive class. Tanh is between 1 and -1, and probability can't be negative. ReLU has a slope of 1 on the right side, so quickly becomes greater than 1, and probability of a class can't be greater than 1."
      ],
      "metadata": {
        "id": "4xf4zc6-cpPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\n"
      ],
      "metadata": {
        "id": "RCvOxA36eL7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\dfrac{\\partial \\mathcal{L}}{\\partial W_j^{(2)}} = \\dfrac{\\partial}{\\partial W_j^{(2)}}C(f(g^{(2)}(h_2))$"
      ],
      "metadata": {
        "id": "_Zb1xrXrre5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$= \\dfrac{\\partial C}{\\partial f} \\dfrac{\\partial f}{\\partial g^{(2)}} \\dfrac{\\partial g^{(2)}}{\\partial W_j^{(2)}}$\n"
      ],
      "metadata": {
        "id": "sfjm4n4Lg0kC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$= (o - y)f'(z)h_j$"
      ],
      "metadata": {
        "id": "T-WhCvvWhwbf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\dfrac{\\partial \\mathcal{L}}{\\partial W_j^{(2)}} = (o - y)f'(z)h_j$"
      ],
      "metadata": {
        "id": "0EhOxyZjcXzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. The network might have a change of very small change in f'(z), which will kill all layers/paths before it, because the change of the cost function is multiplied by every other layer during backpropogation, making all gradients 0. Basically, vanishing gradients."
      ],
      "metadata": {
        "id": "FBBJR_vfra9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. a. The log likelihood maximizes probability of some outcome, so inverting it allows us to minimize in order to do our optimization."
      ],
      "metadata": {
        "id": "wvCwjXhosPEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. b. The range of the cross entropy log function is $[0, \\infty)$. If the two are different (0 and 1), the result is infinity. If they are the same, then the result is 0."
      ],
      "metadata": {
        "id": "rrEHP5CEdEM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. $\\dfrac{\\partial \\mathcal{L}}{\\partial W_j^{(2)}} = \\dfrac{\\partial L}{\\partial f} \\dfrac{\\partial f}{\\partial g^{(2)}} \\dfrac{\\partial g^{(2)}}{\\partial W_j^{(2)}}$"
      ],
      "metadata": {
        "id": "NEUU6StXz2AA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\dfrac{\\partial L}{\\partial f} = (-\\frac{y}{o} + \\frac{1-y}{1-o})$"
      ],
      "metadata": {
        "id": "C2bp2b8LmOKt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\dfrac{\\partial \\mathcal{L}}{\\partial W_j^{(2)}} = (-\\frac{y}{o} + \\frac{1-y}{1-o}) f'(z)h_j = (-\\frac{y}{o} + \\frac{1-y}{1-o}) o(1-o) h_j$"
      ],
      "metadata": {
        "id": "XWzEWmGNkYoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ = (-y(1-o) + o(1-y))h_j = (-y+yo + o-yo)h_j$"
      ],
      "metadata": {
        "id": "dv58WxOTmHGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$= (o - y)h_j$"
      ],
      "metadata": {
        "id": "-gKNywtumi-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. So there clearly isn't a problem anymore with vanishing gradients. We don't have any gradients which can go to zero, even if we're somewhat certain, we don't pollute our other paths due to high certainty in one path."
      ],
      "metadata": {
        "id": "9trOb6c7l_X3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. We use linear activation for regression because it wont have a zero gradient. We can use squared error, as we're just looking to find the distance error in our prediction."
      ],
      "metadata": {
        "id": "Q8GO5zbGk1Ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3. Multilayer Perceptron"
      ],
      "metadata": {
        "id": "Czk9jP17n_0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. $\\dfrac{\\partial \\mathcal{L}}{\\partial W_{21}^{(1)}} = \\dfrac{\\partial C}{\\partial f} \\dfrac{\\partial f}{\\partial g^{(2)}} \\dfrac{\\partial g^{(2)}}{\\partial f_1^{(1)}}\\dfrac{\\partial f_1^{(1)}}{\\partial g_1^{(1)}}\\dfrac{\\partial g_1^{(1)}}{\\partial W_{21}^{(1)}}$"
      ],
      "metadata": {
        "id": "5E69vKvopQmw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\dfrac{\\partial C}{\\partial f} = \\dfrac{\\partial}{\\partial f} (-y log(o) - (1 - y) log(1 - o)) = (-\\frac{y}{o} + \\frac{1-y}{1-o})$"
      ],
      "metadata": {
        "id": "ZPGarvp85mnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\dfrac{\\partial \\mathcal{L}}{\\partial W_{21}^{(1)}} = (-\\frac{y}{o} + \\frac{1-y}{1-o}) f'(z) W_1^{(2)} f'(g_1^{(1)})x_2$"
      ],
      "metadata": {
        "id": "Fq6VPMhX7IT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ = (-\\frac{y}{o} + \\frac{1-y}{1-o}) o(1-o) W_1^{(2)} f(g_1^{(1)}(x))(1 - f(g_1^{(1)}(x)))x_2$"
      ],
      "metadata": {
        "id": "y06s6seB8fxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ = (o - y)W_1^{(2)} f(g_1^{(1)}(x))(1 - f(g_1^{(1)}(x)))x_2$"
      ],
      "metadata": {
        "id": "yudNULfF8257"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. If those conditions are true, that means we are certain about our answer within those paths. So, we the associated bias term must be zero, $\\dfrac{\\partial \\mathcal{L}}{\\partial b_{2}^{(1)}} = 0$ and the weights feeding in would also likely not change either. There isn't enough room behind $h_2^{1}$ to update it, so weights $\\dfrac{\\partial \\mathcal{L}}{\\partial W_{12}^{(1)}} = 0, \\dfrac{\\partial \\mathcal{L}}{\\partial W_{22}^{(1)}} = 0$ as they are killed off by the two weight gradients in the second layer feeding into them vanishing."
      ],
      "metadata": {
        "id": "B7WDrNrA9U7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. This isn't necessarily true if only one weight gradient is 0, because then there is a path to update each weight and the bias mentioned above, and so their gradients wont vanish."
      ],
      "metadata": {
        "id": "Ibr36walIJFz"
      }
    }
  ]
}